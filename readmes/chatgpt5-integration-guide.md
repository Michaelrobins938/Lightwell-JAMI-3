# üß© ChatGPT-5 UI Clone (1:1 Parity) ‚Äî Complete Integration Guide

This project is a **modular, plug-and-play clone of ChatGPT-5's interface** designed for **web, desktop, and mobile deployment**. Every feature from the official app is broken into self-contained modules that can be reused across platforms.

## üìÇ Project Structure & Architecture

```
src/
 ‚îú‚îÄ components/
 ‚îÇ   ‚îú‚îÄ AppShell.tsx              # Global frame: sidebar + header + chat + input bar
 ‚îÇ   ‚îú‚îÄ sidebar/                  # Left column components
 ‚îÇ   ‚îÇ   ‚îú‚îÄ Sidebar.tsx           # Main sidebar wrapper
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ChatHistoryList.tsx   # Past conversations list
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ChatHistoryItem.tsx   # Individual chat row
 ‚îÇ   ‚îÇ   ‚îî‚îÄ SidebarFooter.tsx     # Profile, settings, upgrade
 ‚îÇ   ‚îú‚îÄ header/                   # Top bar components
 ‚îÇ   ‚îÇ   ‚îú‚îÄ Header.tsx            # Header container
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ModelSelector.tsx     # GPT-4o, GPT-5 dropdown
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ShareButton.tsx       # Share modal trigger
 ‚îÇ   ‚îÇ   ‚îî‚îÄ OverflowMenu.tsx      # Three-dots menu
 ‚îÇ   ‚îú‚îÄ chat-window/              # Main conversation area
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ChatWindow.tsx        # Messages container
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ChatMessage.tsx       # Individual message bubble
 ‚îÇ   ‚îÇ   ‚îú‚îÄ MarkdownRenderer.tsx  # Markdown formatting
 ‚îÇ   ‚îÇ   ‚îú‚îÄ CodeBlock.tsx         # Syntax highlighting + copy
 ‚îÇ   ‚îÇ   ‚îî‚îÄ MessageActions.tsx    # Thumbs up/down feedback
 ‚îÇ   ‚îú‚îÄ input-bar/                # User input area
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ChatInput.tsx         # Text input + send
 ‚îÇ   ‚îÇ   ‚îú‚îÄ AttachmentButton.tsx  # + button for files
 ‚îÇ   ‚îÇ   ‚îú‚îÄ VoiceButton.tsx       # Mic button
 ‚îÇ   ‚îÇ   ‚îî‚îÄ AttachmentPreview.tsx # File/audio preview chips
 ‚îÇ   ‚îú‚îÄ modals/                   # Secondary overlays
 ‚îÇ   ‚îÇ   ‚îú‚îÄ FileModal.tsx         # Upload files/attachments
 ‚îÇ   ‚îÇ   ‚îú‚îÄ ConnectorDrawer.tsx   # Google Drive, Notion, Slack
 ‚îÇ   ‚îÇ   ‚îú‚îÄ SettingsModal.tsx     # Theme, shortcuts, preferences
 ‚îÇ   ‚îÇ   ‚îú‚îÄ UpgradeModal.tsx      # Pricing tiers
 ‚îÇ   ‚îÇ   ‚îî‚îÄ ProfileDrawer.tsx     # Account management
 ‚îÇ   ‚îú‚îÄ voice/                    # Voice mode components
 ‚îÇ   ‚îÇ   ‚îú‚îÄ VoiceRecorder.tsx     # Mic capture UI
 ‚îÇ   ‚îÇ   ‚îî‚îÄ VoiceOverlay.tsx      # Floating voice panel
 ‚îÇ   ‚îî‚îÄ shared/                   # Reusable atoms
 ‚îÇ       ‚îú‚îÄ Icon.tsx              # Icon wrapper (Lucide/Heroicons)
 ‚îÇ       ‚îú‚îÄ Button.tsx            # Consistent button styles
 ‚îÇ       ‚îî‚îÄ Loader.tsx            # Loading spinners
 ‚îú‚îÄ hooks/                        # Smart logic layer
 ‚îÇ   ‚îú‚îÄ useChat.ts                # Messages, API calls, streaming
 ‚îÇ   ‚îú‚îÄ usePlan.ts                # Free/Pro/Enterprise gating
 ‚îÇ   ‚îú‚îÄ useModal.ts               # Global modal state management
 ‚îÇ   ‚îú‚îÄ useTheme.ts               # Light/dark/system themes
 ‚îÇ   ‚îú‚îÄ useVoiceSession.ts        # WebRTC to OpenAI Realtime API
 ‚îÇ   ‚îî‚îÄ useRecorder.ts            # Browser audio capture
 ‚îú‚îÄ lib/
 ‚îÇ   ‚îú‚îÄ config.ts                 # API keys, models, plan tiers
 ‚îÇ   ‚îú‚îÄ theme.ts                  # Tailwind theme overrides
 ‚îÇ   ‚îî‚îÄ constants.ts              # Shortcuts, limits, error messages
 ‚îú‚îÄ pages/                        # Next.js routes
 ‚îÇ   ‚îú‚îÄ chat.tsx                  # Main chat interface
 ‚îÇ   ‚îú‚îÄ settings.tsx
 ‚îÇ   ‚îú‚îÄ profile.tsx
 ‚îÇ   ‚îú‚îÄ upgrade.tsx
 ‚îÇ   ‚îî‚îÄ about.tsx
 ‚îî‚îÄ styles/                       # Global CSS + Tailwind
```

## üîë Core Integration Principles

### 1. **Dumb Components, Smart Hooks**
- **Components** = pure render functions (no API calls, no state logic)
- **Hooks** = all business logic (streaming, authentication, plan checks)
- **Example**: `ChatWindow` only renders `messages[]`, while `useChat` manages the conversation

### 2. **AppShell as the Glue**
- `AppShell.tsx` wires together Sidebar, Header, ChatWindow, and InputBar
- Acts as the single source of layout and state coordination
- Passes context from hooks down to dumb components

### 3. **Single Source of Configuration**
- All settings live in `lib/config.ts` (API keys, model lists, plan tiers)
- Swap providers (OpenAI ‚Üí Anthropic) by editing one file
- Environment-specific overrides via `.env.local`

## üåê Platform Integration Patterns

### Web (Next.js + TailwindCSS)
```bash
# Install dependencies
npm install

# Add API key
echo "OPENAI_API_KEY=sk-xxxx" > .env.local

# Run development server
npm run dev

# Visit localhost:3000/chat
```

**Result**: Fully functional ChatGPT-style web UI with all features.

### Desktop (Electron Wrapper)
```bash
# Build web assets
npm run build

# Launch with Electron
npm run desktop
```

**Features Added**:
- System tray icon
- Global hotkey (Cmd+Shift+Space)
- Native file picker dialogs
- Same UI as web, packaged as DMG/EXE

### Mobile (React Native + Expo)
```bash
cd mobile
npm install
npx expo start
```

**Platform Differences**:
- Sidebar ‚Üí slide-in drawer
- Input bar ‚Üí always pinned to bottom
- Floating "+" button opens file/voice modal
- Bottom tab navigation: Chats | Explore | Settings | Profile
- Voice ‚Üí press-and-hold mic with native API

## üóÇÔ∏è Module Assembly Instructions (IKEA-Style)

### Step 1: Global Shell + Layout
**File**: `src/components/AppShell.tsx`

```tsx
export default function AppShell() {
  const { messages, sendMessage } = useChat();
  const { isVoiceActive } = useVoiceSession();

  return (
    <div className="h-screen flex">
      {/* Left Sidebar */}
      <Sidebar />

      {/* Main Content */}
      <div className="flex-1 flex flex-col">
        <Header />
        <ChatWindow messages={messages} />
        <InputBar onSend={sendMessage} isVoiceActive={isVoiceActive} />
      </div>

      {/* Modals */}
      <ModalManager />
    </div>
  );
}
```

**Purpose**: Creates the skeleton that all other components bolt onto.

### Step 2: Wire the Brain (Hooks)
**File**: `src/pages/chat.tsx`

```tsx
import AppShell from '../components/AppShell';

export default function ChatPage() {
  return (
    <ChatProvider>
      <ModalProvider>
        <ThemeProvider>
          <AppShell />
        </ThemeProvider>
      </ModalProvider>
    </ChatProvider>
  );
}
```

**At this stage**: UI renders empty boxes. No functionality yet.

### Step 3: Attach Sidebar
**Wire into**: `AppShell.tsx`

```tsx
import { useChat } from '../hooks/useChat';

function AppShell() {
  const { chats, createNewChat } = useChat();

  return (
    <div>
      <Sidebar chats={chats} onNewChat={createNewChat} />
      {/* ... rest of layout */}
    </div>
  );
}
```

**Behavior**: Sidebar shows chat history, new chat button works.

### Step 4: Attach Header
**Wire into**: `AppShell.tsx`

```tsx
function AppShell() {
  const { activeModel, setActiveModel } = useChat();
  const { openModal } = useModal();

  return (
    <div>
      <Header
        activeModel={activeModel}
        onModelChange={setActiveModel}
        onShare={() => openModal('share')}
        onSettings={() => openModal('settings')}
      />
      {/* ... rest of layout */}
    </div>
  );
}
```

**Behavior**: Model selector, share button, overflow menu functional.

### Step 5: Implement Chat Core
**Wire into**: `AppShell.tsx`

```tsx
function AppShell() {
  const { messages, sendMessage, isStreaming } = useChat();

  return (
    <div>
      <ChatWindow messages={messages} isStreaming={isStreaming} />
      <InputBar onSend={sendMessage} />
    </div>
  );
}
```

**Behavior**: Text conversations work, markdown renders, streaming text.

### Step 6: Add File Attachments
**Wire into**: `InputBar` + `useChat`

```tsx
// In InputBar
<AttachmentButton onFileSelect={handleFileSelect} />

// In useChat
function sendMessage(text: string, files?: File[]) {
  // Upload files, attach to message
  const attachments = await uploadFiles(files);
  // Send to API with attachments
}
```

**Behavior**: File uploads, preview chips, drag-and-drop.

### Step 7: Add Voice Mode
**Wire into**: `InputBar` + new hooks

```tsx
// In InputBar
<VoiceButton onClick={() => setVoiceActive(true)} />

// In AppShell
{isVoiceActive && (
  <VoiceOverlay
    onStop={handleVoiceStop}
    transcript={voiceTranscript}
  />
)}
```

**Behavior**: Mic capture, realtime transcription, audio playback.

### Step 8: Add All Modals
**Wire into**: `ModalManager` component

```tsx
function ModalManager() {
  const { activeModal, closeModal } = useModal();

  return (
    <>
      {activeModal === 'settings' && <SettingsModal onClose={closeModal} />}
      {activeModal === 'profile' && <ProfileDrawer onClose={closeModal} />}
      {activeModal === 'upgrade' && <UpgradeModal onClose={closeModal} />}
      {activeModal === 'file' && <FileModal onClose={closeModal} />}
    </>
  );
}
```

**Behavior**: All secondary screens (settings, profile, upgrade) functional.

### Step 9: Polish & Plan Gating
**Wire into**: `usePlan` hook

```tsx
function AppShell() {
  const { userPlan, hasFeature } = usePlan();

  return (
    <div>
      {hasFeature('voice') && <VoiceButton />}
      {hasFeature('gpt5') && <ModelSelector />}
      {/* ... conditional rendering based on plan */}
    </div>
  );
}
```

**Behavior**: Free/Pro/Enterprise feature toggles.

## üéôÔ∏èüîä Voice Mode Deep Integration (OpenAI Realtime API)

### Parts Required
- `useVoiceSession.ts` ‚Äî WebRTC connection management
- `useRecorder.ts` ‚Äî Browser audio capture
- `useAudioPlayer.ts` ‚Äî Playback of assistant audio
- `VoiceButton.tsx` + `VoiceOverlay.tsx` ‚Äî UI controls

### Step-by-Step Voice Assembly

#### 1. Add Voice Button
```tsx
// input-bar/VoiceButton.tsx
export default function VoiceButton({ onClick }) {
  return (
    <button onClick={onClick}>
      <MicIcon />
    </button>
  );
}
```

#### 2. Create Voice Overlay
```tsx
// voice/VoiceOverlay.tsx
export default function VoiceOverlay({ transcript, onStop, isAssistantSpeaking }) {
  return (
    <div className="fixed inset-0 bg-black/50 flex items-center justify-center">
      <div className="bg-white rounded-lg p-6 max-w-md w-full mx-4">
        <WaveformVisualizer />
        <p className="text-lg">{transcript}</p>
        {isAssistantSpeaking && <p>Assistant speaking...</p>}
        <button onClick={onStop}>Stop Recording</button>
      </div>
    </div>
  );
}
```

#### 3. Implement Audio Recorder
```tsx
// hooks/useRecorder.ts
export function useRecorder() {
  const [stream, setStream] = useState<MediaStream | null>(null);

  async function start() {
    const mic = await navigator.mediaDevices.getUserMedia({
      audio: { sampleRate: 16000, channelCount: 1 }
    });
    setStream(mic);
    return mic;
  }

  function stop() {
    stream?.getTracks().forEach(track => track.stop());
    setStream(null);
  }

  return { start, stop, stream };
}
```

#### 4. Implement Voice Session (WebRTC)
```tsx
// hooks/useVoiceSession.ts
export function useVoiceSession() {
  const pc = useRef<RTCPeerConnection | null>(null);
  const [transcript, setTranscript] = useState("");
  const [isAssistantSpeaking, setIsAssistantSpeaking] = useState(false);

  async function connect(micStream: MediaStream) {
    pc.current = new RTCPeerConnection();

    // Add microphone track
    pc.current.addTrack(micStream.getTracks()[0], micStream);

    // Listen for assistant's audio track
    pc.current.ontrack = (event) => {
      const remoteStream = event.streams[0];
      setIsAssistantSpeaking(true);
      // Play remote audio stream
      playAudioStream(remoteStream);
    };

    // Create data channel for transcript events
    const dc = pc.current.createDataChannel("oai-events");
    dc.onmessage = (event) => {
      const msg = JSON.parse(event.data);
      if (msg.type === "response.text.delta") {
        setTranscript(prev => prev + msg.delta);
      } else if (msg.type === "response.text.completed") {
        // Final transcript ready
        onFinalTranscript(msg.text);
      }
    };

    // Create WebRTC offer
    const offer = await pc.current.createOffer();
    await pc.current.setLocalDescription(offer);

    // Get short-lived token from server
    const tokenResp = await fetch('/api/session');
    const { client_secret } = await tokenResp.json();

    // Send offer to OpenAI Realtime API
    const response = await fetch(
      `https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${client_secret}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          sdp: offer.sdp,
          type: offer.type
        })
      }
    );

    const answer = await response.json();
    await pc.current.setRemoteDescription(answer);
  }

  function disconnect() {
    pc.current?.close();
    pc.current = null;
    setTranscript("");
    setIsAssistantSpeaking(false);
  }

  return { connect, disconnect, transcript, isAssistantSpeaking };
}
```

#### 5. Add Audio Playback Hook
```tsx
// hooks/useAudioPlayer.ts
export function useAudioPlayer() {
  const audioRef = useRef<HTMLAudioElement | null>(null);

  function playStream(stream: MediaStream) {
    if (!audioRef.current) {
      audioRef.current = new Audio();
      audioRef.current.autoplay = true;
    }
    audioRef.current.srcObject = stream;
    audioRef.current.play();
  }

  function stop() {
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.srcObject = null;
    }
  }

  return { playStream, stop };
}
```

#### 6. Wire Voice into Main App
```tsx
// In AppShell.tsx or main chat component
function ChatContainer() {
  const { start, stop, stream } = useRecorder();
  const { connect, disconnect, transcript, isAssistantSpeaking } = useVoiceSession();
  const { sendMessage } = useChat();
  const [isVoiceActive, setIsVoiceActive] = useState(false);

  async function startVoiceRecording() {
    const micStream = await start();
    setIsVoiceActive(true);
    await connect(micStream);
  }

  function stopVoiceRecording() {
    stop();
    disconnect();
    setIsVoiceActive(false);
    // Send final transcript as message
    if (transcript) {
      sendMessage(transcript);
    }
  }

  return (
    <div>
      <InputBar
        onVoiceClick={startVoiceRecording}
        hasVoice={true}
      />

      {isVoiceActive && (
        <VoiceOverlay
          transcript={transcript}
          onStop={stopVoiceRecording}
          isAssistantSpeaking={isAssistantSpeaking}
        />
      )}
    </div>
  );
}
```

## üìê Complete Voice Flow Blueprint

```
User Action: Tap Mic
     ‚Üì
VoiceButton ‚Üí startVoiceRecording()
     ‚Üì
useRecorder.start() ‚Üí getUserMedia()
     ‚Üì
useVoiceSession.connect(micStream)
     ‚Üì
WebRTC Session Established
     ‚Üì
Audio Streams Bidirectionally
     ‚Üì
OpenAI Realtime API
     ‚Üô          ‚Üò
Transcript Deltas    Assistant Audio
     ‚Üì          ‚Üì
VoiceOverlay       useAudioPlayer
     ‚Üì          ‚Üì
Live Text Display  Speaker Output
     ‚Üì
User: "Stop"
     ‚Üì
Final Transcript ‚Üí useChat.sendMessage()
     ‚Üì
ChatWindow (text conversation continues)
```

## ‚úÖ Final 1:1 Parity Checklist

### Core Features
- [ ] Sidebar resizes + collapses on mobile
- [ ] Header with model selector, share, overflow menu
- [ ] Input bar floats initially, pins after first message
- [ ] Token-by-token streaming text
- [ ] Markdown + code blocks with copy button
- [ ] File upload + preview chips
- [ ] Voice recording + realtime transcription
- [ ] All modals: settings, profile, upgrade, connectors
- [ ] Plan gating: Free vs Pro vs Enterprise
- [ ] Theme switching: light/dark/system
- [ ] Keyboard shortcuts modal

### Voice Mode Specific
- [ ] Mic permission request
- [ ] WebRTC session to OpenAI Realtime
- [ ] Live transcript display
- [ ] Assistant audio playback
- [ ] Session cleanup on stop
- [ ] Final transcript injection into chat

## üß≠ Mental Model for Agents

**Hooks = Brain**: All logic, API calls, state management
**Components = Face**: Pure render functions, receive props only
**AppShell = Skeleton**: Wires everything together
**Config = DNA**: Single source of truth for all settings
**Web/Desktop/Mobile = Different Bodies**: Same brain/face, different skin

## üìù Rules for AI Agents (Refactoring/Extending)

1. **Always specify the target module** in requests
   - Example: *"Add video attachment support in `AttachmentPreview.tsx` and extend `useChat.ts`"*

2. **Keep UI components dumb, hooks smart**
   - Never put API calls or state logic in components
   - Components should be pure functions of props

3. **Follow the export convention**
   - Each folder has `index.ts` that re-exports its parts
   - Use default exports for main components

4. **No placeholders, everything functional**
   - Every component, hook, and config must be immediately usable

5. **Single responsibility principle**
   - One component/hook does one thing well
   - Compose complex behavior from simple parts

## üöÄ Deployment & Distribution

### Web Deployment
```bash
# Build optimized bundle
npm run build

# Deploy to Vercel/Netlify
vercel --prod
```

### Desktop Packaging
```bash
# Build web assets
npm run build

# Package with Electron
npm run electron:build

# Results: DMG (macOS), EXE (Windows), AppImage (Linux)
```

### Mobile Build
```bash
# React Native + Expo
cd mobile
npx expo build:ios
npx expo build:android

# Results: IPA (iOS), APK (Android)
```

## üîÑ Maintenance & Extension Patterns

### Adding New Features
1. **Identify the right module** (UI, hook, or config)
2. **Extend the hook** with new logic
3. **Pass data down** through AppShell to components
4. **Update config** if needed
5. **Test across platforms**

### Common Extension Patterns
- **New AI model** ‚Üí Update `lib/config.ts` + `ModelSelector.tsx`
- **New attachment type** ‚Üí Extend `AttachmentPreview.tsx` + `useChat.ts`
- **New modal** ‚Üí Add to `modals/` folder + `useModal.ts`
- **New plan feature** ‚Üí Update `usePlan.ts` + conditional rendering

## üìö Further Reading

- [OpenAI Realtime API Documentation](https://platform.openai.com/docs/guides/realtime)
- [WebRTC MDN Guide](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API)
- [Next.js App Router](https://nextjs.org/docs/app)
- [Tailwind CSS](https://tailwindcss.com/docs)

---

This integration guide serves as your **IKEA instruction manual** for the ChatGPT-5 clone. Each component is a LEGO brick that snaps together following these patterns. When extending the system, always follow the **dumb components, smart hooks** principle and keep the AppShell as your central wiring point.
